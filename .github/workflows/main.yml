# name: Scrape latest data
on:
 workflow_dispatch:
 schedule:
   - cron:  '0 */12 * * *'
   # how often you want program to run - cronjob format minutes hours days months years */12 means every 12 hours

#permissions for the cronjob
permissions:
  contents: write
  packages: write
  pull-requests: write

jobs:
 scrape:
   runs-on: ubuntu-latest
   steps:
   # Step 1: Prepare the environment - tell server to check out the repo through fetch
   - name: Check out this repo
     uses: actions/checkout@v2
     with:
       fetch-depth: 0
       
   # Step 2: Install requirements, so Python script can run - installing software from requirements.txt file
   - name: Install requirements
     # this may change depending on how we handle dependencies
     run: pip install -r requirements.txt


   # Step 3   
   - name: Run scraper

  # Optional: If you are publishing to Google sheets, uncomment these next 2 lines
     #env: 
      #GOOGLE_SECRET_KEY: ${{ secrets.GOOGLE_SECRET_KEY }}

     run: jupyter execute pipeline.ipynb

   # Step 4   
   - name: Commit files back to repository make sure your repository name is correct
     run: |
       git config remote.origin.url https://github.com/roguemodetech/live-pipeline.git
       git config --global user.name "$(git --no-pager log --format=format:'%an' -n 1)"
       git config --global user.email "$(git --no-pager log --format=format:'%ae' -n 1)"
       git add .
       git commit -m "Automated commit from GitHub Actions" || exit 0
       git pull
       git push 
    
